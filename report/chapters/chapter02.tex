\chapter{Dataset Generation}

\section{Introduction}
\section{Analysis}
Generating a Mean Payoff Game can be decomposed into two subsequent objectives.
\begin{enumerate}
	\item Generate the Graph itself.
	\item Generate the Weights
\end{enumerate}


\section{Graph Distributions}
There are many well studied graph distributions in the litterature. \newline
One of the most explored ones are the $\mathcal{G}(n,p)$ and $\mathcal{G}(n,m)$ families.
\subsection{$\mathcal{G}(n,p)$ Family}
For $n\in\mathbb{N},p\in[0,1],$ a graph $G$ is said to follow a $\mathcal{G}(n,p)$ distribution if $\lvert V \rvert=n$ and:
$$
\forall e\in \mathscr{E}, \quad \mathscr{P}(s\in \mathcal{E})=p
$$
Where $\mathscr{E}$ is a set of valid edges. 

\subsection{$\mathcal{G}(n,m)$ Family}
For $n\in\mathbb{N},m\in\mathbb{N},$ a graph $G$ is said to follow a $\mathcal{G}(n,m)$ distribution if $\lvert V \rvert=n,\lvert  E \rvert=m$ and the edges $e_1,\dots,e_m$ were drawn from a set of valid edges $\mathscr{E}.$
\subsection{Valid edges}
The set of valid edges $\mathscr{E}$ is the set defining the potential edges of the graph. It is equal to:
\begin{enumerate}
	\item $V\times V$ for directed graphs with loops 
	\item $(V\times V)\setminus V\odot V$ for directed graphs without loops
	\item The set of subsets of size $2$ of $V$ denoted $\mathscr{P}_2(V)$ for undirected graphs with loops.
	\item The set of subsets of size $2$ of $V$ denoted $\mathscr{P}_2(V)$ for undirected graphs with loops.
\end{enumerate}
\subsection{$\mathcal{D}(n,p)$ Graph Construction}

\subsubsection{Naive Method}
The definition of $\mathcal{D}(n,p)$ gives a straightforward construction. \newline
This is achieved by flipping a coin\footnote{The coin is potentially biased with a probability of obtaining head equal to $p\in [0,1]$} for each pair of node $(u,v)\in V^2$, we add an edge if we get a Head. 
\newline This is implemented in the following algorithm:
\begin{algorithm}
	\caption{$\mathcal{D}(n,p)$ Graph Generation}\label{alg:Dnp_Naive}
	\begin{algorithmic}
		\Require $n\in\mathbb{N}^*$ the size of the graph
		\Require $p\in\mathbb{N}^*$ the edge probability 
		\Ensure $G\sim \mathcal{D}(n,p)$  
		\State $A:(u,v)\in V\times V\rightarrow 0$
		\For{ $u\in V$} 
		\For { $v \in V$ }
		\State Generate $X\sim \mathcal{B}(p)$
		\Comment{$\mathcal{B}(p)$ is the bernoulli distribution}
		\State $A(u,v)\leftarrow X$
		\EndFor
		\EndFor
		\State \Return $G\leftarrow \texttt{GraphFromAdjacencyMatrix}(A)$
	\end{algorithmic}
\end{algorithm}
\FloatBarrier
The complexity\footnote{We assume the cost of generating a Bernoulli random variable as $\mathcal{O}(1)$} of the following algorithm is $\mathcal{O}(n^2).$
\subsubsection{Optimized Method}
Instead of iterating over all possible pair of nodes. For each vertex $v\in V$:
\begin{itemize}
	\item We can sample a number $d$ from the outgoing degree distribution\footnote{Or the ingoing degree distribution, they are in fact equal.}
	\item We then choose $d$ numbers uniformly without replacement from an indexable representation of $V$
\end{itemize}
The following algorithm implements the optimized method:
\begin{algorithm}
	\caption{$\mathcal{D}(n,p)$ Graph Generation Optimisation}\label{alg:Dnp_Fast}
	\begin{algorithmic}
		\Require $n\in\mathbb{N}^*$ the size of the graph
		\Require $p\in\mathbb{N}^*$ the edge probability 
		\Ensure $G\sim \mathcal{D}(n,p)$  
		\State $A:u\in V\rightarrow \varnothing$
		\For{ $u\in V$} 
		\State Generate $d\sim \mathcal{B}(n,p)$
		\Comment{$d$ represents the degree, $\mathcal{B}(n,p)$ is the binomial distribution}
		\State $A(u)\leftarrow \choice(V,d)$
		\EndFor
		\State \Return $G\leftarrow \texttt{GraphFromAdjacencyList}(A)$
	\end{algorithmic}
\end{algorithm}
\FloatBarrier
Now, Let $C(n,m)$ be the cost of choice function. The expected complexity of this algorithm will be:
$$
\tilde{\mathcal{O}}\left(n\mathbb{E}_d[C(n,d)]\right) \quad \text{where}\ d\sim \mathcal{B}(n,p)
$$
We will show on the next section what choice function should we use.
\subsection{Choice Function}
\subsubsection{First Proposition}
We propose here a simple choice algorithm, but it is still efficient for our use case.
\newline It works simply by drawing without replacement, but we ignore duplicate elements. This is implemented as follow
\begin{algorithm}
	\caption{$\mathcal{D}(n,p)$ Choice without replacement}\label{alg:Choice}
	\begin{algorithmic}
		\Require $S$ a list
		\Require $m\in\{0,\dots \lvert S \rvert\}$ the number of chosen elements
		\Ensure $H$ a set of size $m$ containing uniformly drawn elements without replacement. 
		\State $H\leftarrow \varnothing$
		\While{$\lvert H \rvert < m$}
			\State Generate $v\sim \mathcal{U}(S)$ 
			\Comment{Where $\mathcal{U}(S)$ is the uniform distribution over $S$}
			\State $H\leftarrow H \cup \{v\}$
		\EndWhile
		\State \Return $H$
	\end{algorithmic}
\end{algorithm}
\FloatBarrier
To estimate the cost of this algorithm, we will use probabilistic reasoning.
\newline Let $X_{n,m}=C(n,m)$ the running time of an execution of algorithm \ref{alg:Choice} in a set $S$ of size $n$, with $m$ elements to be chosen.
We have:
\begin{align*}
	X_{n,0} & \ \text{is deterministic}\\
	X_{n,0}&=\mathcal{O}(1) \\
	\mathbb{E}[X_{n,m}]&=1+\frac{1}{n}\sum_{k=0}^{n-1} \mathbb{E}[X_{n,m} \mid \text{The last drawn number is}\ k] \\
	&=1+\frac{1}{n}\sum_{k=0}^{m-2} \mathbb{E}[X_{n,m}]+\frac{1}{n}\sum_{k=m-1}^{n-1} \mathbb{E}[X_{n,m-1}] \\
	&= 1+\frac{m-1}{n}\mathbb{E}[X_{n,m}]+\frac{n-m+1}{n}\mathbb{E}[X_{n,m-1}]
\end{align*}
Now we arrived at a recurrent formula. We will simplify it as shown below:
\begin{align*}
\frac{n-m+1}{n}\mathbb{E}[X_{n,m}]&=\frac{n-m+1}{n}\mathbb{E}[X_{n,m-1}] +1\\
\implies \mathbb{E}[X_{n,m}]&=\frac{n-m+1}{n-m+1}\mathbb{E}[X_{n,m-1}]+\frac{n}{n-m+1}\\
&=\mathbb{E}[X_{n,m-1}]+\frac{n}{n-m+1} \\
&=\sum_{k=1}^m\frac{n}{n-k+1}+\mathcal{O}(1)\\
&=\sum_{k=0}^{m-1}\frac{n}{n-k}+\mathcal{O}(1) \\
&=n\sum_{k=n-m+1}^n\frac{1}{k}+\mathcal{O}(1)\\
&=n(H_n-H_{n-m})+\mathcal{O}(1)
\end{align*}
Here $(H)_{n\in\mathbb{N}^*}$ is the harmonic series, and we define $H_0=0.$
\subsubsection{Complexity}
%Prooof!!!
The expected complexity of algorithm \ref{alg:Choice} depends on both $n$ and $m$:
\begin{itemize}
	\item If $m=o(n),$ then it is $\tilde{\mathcal{O}}(m).$
	\item If $m=kn+o(n)$ with $k\in\mathopen]0,1\mathclose[$, then it is $\tilde{\mathcal{O}}(m).$
	\item If $m=n-o(n)$, It is\footnote{Here we use the minus sign to emphasize that $m\le n$} $\tilde{\mathcal{O}}(m\log m).$ 
\end{itemize}
To prove this result, we use a well known asymptotic approximation of the Harmonic series\footnote{This asymptotic approximation can be proven using the Eulerâ€“Maclaurin formula}:
$$
H_n=\ln n+\gamma -\frac{1}{2n}+\mathcal{O}\left(\frac{1}{n^2}\right)
$$
We can prove this claim as follow:
\begin{align*}
	m=o(n),\quad \mathbb{E}[C(n,m)]&=-n\ln \left(1-\frac{m}{n}\right) -\frac{1}{2}\left(1-\frac{n}{n-m}\right)+\mathcal{O}\left(\frac{1}{n}\right) \\
	&=m+o(m)\\
	&=\mathcal{O}(m)\\
	m=km+o(m),k\in \mathopen]0,1\mathclose[,\quad \mathbb{E}[C(n,m)]&=-n\ln \left(1-\frac{m}{n}\right) -\frac{1}{2}\left(1-\frac{n}{n-m}\right)+\mathcal{O}\left(\frac{1}{n}\right) \\
&=-n\ln (1-k+o(1))+\frac{1}{n}(1-\tfrac{1}{1-k+o(1)})+\mathcal{O}(\tfrac{1}{n})\\
&=\mathcal{O}(m)\\
\end{align*}
For $m=n-o(n),$ we prove it by noting that:
\begin{align*}
\mathbb{E}[C(n,m)]&\le\mathbb{E}[C(n,n)] \\
 &\le nH_n \\
&\le n\ln n +\gamma n +\frac{1}{2}+\mathcal{O}\left(\frac{1}{n}\right) \\
&= \mathcal{O}(m\log m)
\end{align*}

\subsubsection{Refinement}
If $m$ tends to $n,$ it is more hard to select $m$ elements from a set of size $n$ without replacement. This explains the extra logarithmic factor.
\newline In that case, we can instead focus on the dual problem: ``Find the $n-m$ elements that will not be selected". This can be calculated in $\mathcal{O}(n-m).$
\newline Once we find the elements that will not be selected, their set complement are exactly the $m$ elements that will be selected. This new algorithm is guaranteed to be $\mathcal{O}(m)$ irrespective of $n$ and $m$
\begin{algorithm}
	\caption{Fine tuned $\mathcal{D}(n,p)$ Choice without replacement }\label{alg:ChoiceFineTuned}
	\begin{algorithmic}
		\Require $S$ a list
		\Require $m\in\{0,\dots \lvert S \rvert\}$ the number of chosen elements
		\Require $\choice$ The choice function defined on algorithm \ref{alg:Choice}
		\Require $\tau$ a fine tuned threshold. We will use $\tau=\frac{1}{2}$ for all practical purposes.
		\Ensure $H$ a set of size $m$ containing uniformly drawn elements without replacement. 
		\If {$\frac{m}{\lvert S \rvert} \le \tau$}
			\State $H\leftarrow \choice(V,n)$
		\Else
			\State $H\leftarrow S\setminus \choice(S,n-m)$
		\EndIf
		\State \Return $H$
	\end{algorithmic}
\end{algorithm}
\FloatBarrier
Also, an important point is that by combining the analysis of all possible cases, we can extract a constant factor that is independent of $n.$ So that the Big-O notation is only a function of $m$.
\subsection{Complexity of Optimised $\mathcal{D}(n,p)$ Graph Construction}
We return to evaluate the asymptotic behaviour of $\mathbb{E}_d[C(n,d)].$
\newline Let $\delta \in \mathbb{R}_+^*$
\newline The Chebychev Inequality implies that:
$$
\mathscr{P}\left(\left \lvert \frac{1}{n}\sum_{i=1}^nX_i -p \right \rvert \ge \frac{\delta}{\sqrt{n}}\sqrt{p(1-p)}  \right) \le \frac{1}{\delta^2}
$$
By setting: $\delta=\frac{1}{\sqrt{p}},$ we have:
$$\mathscr{P}\left(\left \lvert \frac{1}{n}\sum_{i=1}^nX_i -p \right \rvert \ge \frac{\sqrt{1-p}}{\sqrt{n}}  \right) \le p 
$$
Let $I=[np-\sqrt{n(1-p)},np+\sqrt{n(1-p)}].$
\newline 
We have:
\begin{align*}
\mathbb{E}\left[C(n,d)\right]  &\le \mathbb{E}\left[C(n,d) \mid d\in I\right] + p^2\mathbb{E}\left[C(n,d) \mid d\notin I\right] \\
&\le \max_{m\in \mathbb{N} \cap I}C(n,m) + \max_{m\in\{0,\dots,n\}}p^2C(n,m) 
 \end{align*}
To further simplify this, we need two key observations:
\begin{itemize}
	\item The interval $m\in \mathbb{N} \cap [np-1+p,np+1-p]$ contains at most $3$ integers, all of which are within a distance of $1$ to $np$
	\item The complexity of $\mathbb{E}[C(n,m)]$ does only depend on $m.$
\end{itemize}
Thus, we have the following:
\begin{align*}
	\max_{m\in \mathbb{N} \cap [np-1+p,np+1-p]}C(n,m) &= \mathcal{O}(np) \\
	\max_{m\in\{0,\dots,n\}}p^2C(n,m) &= \mathcal{O}(np^2)
\end{align*}
Now, by combining both estimations, we get:
$$
\tilde{\mathcal{O}}(n\mathbb{E}_d[C(n,d)])=\tilde{\mathcal{O}}(n^2p) \quad \blacksquare
$$

\subsection{$\mathcal{D}(n,m)$ Construction}
To construct a random $\mathcal{D}(n,m)$ graph, we only have to select $m$ uniformly random elements from the set $V\times V.$
\newline We will use algorithm \ref{alg:ChoiceFineTuned} for this purpose\footnote{It is essential that the list $V\times V$ be lazy loaded. In particular, each element will only be loaded when it is indexed. This is essential to reduce the complexity. Otherwise, we will be stuck in an $\mathcal{O}(n^2)$ algorithm.}:
\begin{algorithm}
	\caption{Fine tuned $\mathcal{D}(n,p)$ Choice without replacement }\label{Dnm} 
	\begin{algorithmic}
		\Require $n\in\mathbb{N}^*$
		\Require $m\in\{0,\dots,n^2\}$ the number of chosen elements
		\Ensure $G\sim \mathcal{D}(n,m)$
		\State $E\leftarrow \choice(\text{Lazy}(V)\times \text{Lazy}(V),m)$ \Comment{We only need the $m$ elements on-demand.}
		\State \Return $G\leftarrow \text{GraphFromEdges}(E)$\Comment{This justifies using \text{Lazy}}
	\end{algorithmic}
\end{algorithm}
\FloatBarrier
Here $\text{Lazy}(V)\times \text{Lazy}(V)$ is a lazy implementation of cartesian product that supports bijective indexing\footnote{Indexing is required for uniform sampling} over $\{0,\dots,n^2-1\}.$
\newline The complexity of this construction is: $
\tilde{\mathcal{O}}(m)
$ 

\section{Sinkless Conditionning}
Sampling from a graph distribution may lead to graphs that have at least one sink. 
\newline These graphs are problematic as Mean Payoff Graphs are exactly the sinkless graphs.
\newline To migitate this, we will impose a conditionning on both distribution that will gives a guaranteed Mean Payoff Graph.
\newline We will explore such conditionning both distribution:
\begin{itemize}
	\item $\mathcal{G}^S(n,p):$ This is the distribution of graphs following $\mathcal{G}(n,p)$ with the requirement that they do not have a sink.
	\item $\mathcal{G}^S(n,m):$ This is the distribution of graphs following $\mathcal{G}(n,m)$ with the requirement that they do not have a sink.
\end{itemize}
\subsection{Repeating Construction}
\subsubsection{Algorithm}
This method is very intuitive. It will repeat the sampling until getting the desired graph. \newline The following is an implemention of the repeating construction.
\begin{algorithm}
	\caption{Fine tuned $\mathcal{D}(n,p)$ Choice without replacement }\label{alg:RepeatingConstruction}
	\begin{algorithmic}
		\Require $n\in\mathbb{N}^*$
		\Require $m\in\{0,\dots \lvert S \rvert\}$ the number of chosen elements
		\Require $\choice$ The choice function defined on algorithm \ref{alg:Choice}
		\Require Threshold $\tau$
		\Ensure $H$ a set of size $m$ containing uniformly drawn elements without replacement. 
		\If {$\frac{m}{\lvert S \rvert} \le \tau$}
		\State $H\leftarrow \choice(V,n)$
		\Else
		\State $H\leftarrow V\setminus \choice(S,n-m)$
		\EndIf
		\State \Return $H$
	\end{algorithmic}
\end{algorithm}

\subsubsection{Analysis}
We will analyse the runtime of generating a $\mathcal{G}^S(n,p).$
\newline We expect a similar runtime for $\mathcal{G}^S(n,m)$ due to the similarity between $\mathcal{G}(n,m)$ and $\mathcal{G}(n,p).$ 
\newline Let $F(n)$


\section{Weights Distribution}
\subsection{Construction}
Once the graph is constructed. We only have to generate the weights. \newline
This will be done by creating a random weight function:
$$
W(u,v):(u,v)\rightarrow W_{u,v}
$$
Here $W_{u,v}$ will be a sequence of real random variables. \newline
In our case, we set $(W_{u,v})_{(u,v)\in E}$ to be independent and identically distributed over a real distribution $\mathcal{W}.$ 


\section{Proposed MPG Distribution}
\subsection{Desired Properties of Mean Payoff Game Distributions}
\subsubsection{Fairness in the Limit}
This is essential, as we intend to generate a sequence of Mean Payoff Games that do not favour statistically a certain player, in the sense that, if we generate sufficient independent and identically distributed Mean Payoff Games $G_1,\dots,G_n$, we expect the following:
$$
\lim_{n\rightarrow +\infty} \left\lvert \mathtt{R}_{\Max}(G_1,\dots,G_n)-\mathtt{R}_{\Min}(G_1,\dots,G_n)\right \rvert = 0
$$
Where $\mathtt{R}$ is defined as follow:
$$
\mathtt{R}_{\text{Op}}(G_1,\dots,G_n)=\frac{1}{n}\sum_{i=1}^n\mathscr{P}(\text{Op wins} \ G_i\ \text{assuming optimal strategies})
$$
\subsubsection{Symmetric}
A real distribution is said to be symmetric if:
$$
\forall [a,b]\in \mathbb{R},X\sim \mathcal{W},\quad \mathscr{P}(X\in [a,b]) = \mathscr{P}(X\in [-b,-a])
$$
We will define a symmetric Mean Payoff Game distribution as a distribution of Mean Payoff Game whose weights are independent and identically distributed on a symmetric real distribution.
This property is stronger than Fairness in the Limit, as it implies that:
$$
\mathscr{P}(\text{Max wins} \ G\ \text{assuming optimal strategies}) = \mathscr{P}(\text{Min wins} \ G\ \text{assuming optimal strategies})
$$
We will require a Symmetric Mean Payoff Game as we do not want a player to have an inherit advantage other the other one\footnote{Other than the first move.}
\subsection{Implemented Distributions}
The following table resumes the implemented distributions:
\begin{table}[h]
	\small
	\begin{tabularx}{\textwidth}{| X | X | X |}
		\hline
		
		Distribution Family & Parameters & Type  \\
		\hline
		$\mathcal{D}(n,p)$ & \vspace{-5mm}
		\begin{itemize}
			  \setlength\itemsep{0em}
			\item $n:$ Graph size
			\item $p:$ Edge probability
		\end{itemize} & Graph distribution \\
		\hline
		$\mathcal{D}(n,m)$ & 
		\vspace{-5mm}
		\begin{itemize}
			  \setlength\itemsep{0em}
			\item $n:$ Graph size
			\item $m:$ Number of edges
		\end{itemize} & Graph distrbiution  \\
		\hline
		$\mathcal{U}_{\text{discrete}}(-r,r)$ &
		\vspace{-5mm}
		\begin{itemize}
			  \setlength\itemsep{0em}
			\item $r:$ The radius of the support
		\end{itemize}
		 &  Weight distribution\\
		\hline
		$\mathcal{U}(-r,r)$ &\vspace{-5mm}
		\begin{itemize}
			  \setlength\itemsep{0em}
			\item $r:$ The radius of the support
		\end{itemize} & Weight distribution \\
		\hline
		$\mathcal{N}(0,\sigma)$ &
		\vspace{-5mm}
		\begin{itemize}
			  \setlength\itemsep{0em}
			\item $\sigma:$ The standard deviation
		\end{itemize} & Weight distribution\\ 
		\hline 
		
	\end{tabularx}
	\caption{Le tableau d'avancement des BNNs}
\end{table}
\FloatBarrier


\section{MPG Generation}
\subsection{Distribution}
\begin{itemize}
	\item Each generated graph will follow a distribution $\mathcal{G}(n,p(n))$  for some $n\in\mathbb{N}^*$
	\item The weights will follow the discrete uniform distribution $\mathcal{D}(-1000,1000)$

\end{itemize}

We will generate two kinds of datasets, depending on the nature of the graph

\subsection{Dense Graphs}
\begin{itemize}
	\item Let $\mathcal{P}=\{0.1,0.2,0.3,0.5,0.7,0.8,0.9,1\}$
	\item $\mathcal{N}=\{10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,200,250,300,400,500\}$
	\item For each $(n,p)\in \mathcal{N}\times \mathcal{P},$ we will generate $K=1000$ observations $G^{n,p}_1,\dots,G^{n,p}_{K} \sim \mathcal{G}(n,p)$ 
\end{itemize} 

The total number of examples is:
$$
K\times\lvert \mathcal{N} \rvert \times \lvert \mathcal{P}\rvert=160000
$$
The generation was done on a `haswell64` partition with 24 cores. and it took 02:12:38 hours.

\section{Annotation}
\subsection{Approach}
We used the CSP algorithm $\ref{alg:AC3Optimized}$ to annotate the dataset, potentially augmented with some heuristics. 
\newline We implemented a program that takes the path of the dataset, and solves the Mean Payoff Games one by one.
\newline To maximize efficiency, the program launches many solver threads, with each one independently working on a single file, and the results are accumulated using a ConcurrentQueue.
\subsection{Target Values}
The solver will calculate the following targets:
\begin{itemize}
	\item The optimal pair of strategies
	\item The mean payoffs for each starting position, turn.
	\item The winners for each starting position, turn.
\end{itemize}
Also, some additional metadata are generated for analysis:
\begin{itemize}
	\item \texttt{dataset}: The name of the whole dataset
	\item \texttt{graph}: The name of the graph.
	\item \texttt{status}: The solver's status on the given graph. In particular, whether it succeeded to solve the instance or not\footnote{We expect that the solver may crash due to several reasons (corrupted file, out of memory, etc\dots). For that we made additional effort for exception handling, so that an error for a single instance does not propagate to the whole program.}. Equal to ``OK" if the execution is successful.
	\item \texttt{running\_time}: The time needed to solve the instance.

\end{itemize}

\subsection{Heuristics}
To accelerate the annotation of the two datasets, we had to apply some heuristics to the algorithm. We made essentially two kinds of heuristics.
\subsubsection{Linear Bound Heuristic}
This is the heuristic based on the view that for almost all solutions of a Ternary Max Atom system extracted from our generated random games, either:
\begin{itemize}
	\item All variables are infinite: 
	$$
	X(u)=-\infty \quad \forall u\in V$$
	\item The diameter of assignments is in the order of $\lVert W\rVert_{\infty}$
	$$
	\Delta X = \sup_{u\in V}X(u)-\inf_{u\in V,X(u)>-\infty}X(u)= \mathcal{O}(\lVert W\rVert_{\infty})$$
\end{itemize}
This heuristic suggests a much tighter search space to the worst case $\lVert W \rVert_{1}$ one. Going further, with uniform random weights:
$$
\lVert W \rVert_{1}=\mathcal{O}\left(\lvert E \rvert \times \lVert W \rVert_{\infty}\right)
$$
We believe this heuristic arises due to the random property of graphs, because in general, one can build an infinite family of ternary max atom systems that violate this heuristic. \newline In fact, going further, one can build ternary max atom systems were the $\lVert W \rVert_{1}$ estimation is tight.
%Example of such system:
% Linear chain of constraints 

To generating the dataset, we applied this heuristic with $\Delta X=4\lVert W\rVert_{\infty}$
$$
D = \{-\infty,-2 \lVert W\rVert_{\infty},,-2 \lVert W\rVert_{\infty}+1 ,\dots ,2\lVert W\rVert_{\infty} \}
$$

\begin{figure}[H]
	\centering
	\begin{tikzpicture}[node distance={20mm}, thick, main/.style = {draw, circle}]
		\node[main] (1) {$0$}; 
		\node[main] (2) [right of =1] {$1$}; 
		\node[main] (3) [right of =2] {$2$}; 
		\node[main] (4) [right of =3] {$3$}; 
		\node[main] (5) [right of =4] {$\dots$}; 
		\node[main] (6) [right of =5] {$n-1$}; 
		\node[main] (7) [right of =6] {$n$}; 
		\draw[->] (1) -- node[midway, above right, sloped, pos=0.3] {-5} (2);
		\draw[->] (2) -- node[midway, above, sloped, pos=0.5]{6} (3);
		\draw[->] (3) -- node[midway, above, sloped, pos=0.5] {-8} (4);
		\draw[->] (4) -- node[midway, above, sloped, pos=0.5] {-5}(5);
		\draw[->] (5) -- node[midway, above, sloped, pos=0.5] {7}(6);
		\draw[->] (6) -- node[midway, above, sloped, pos=0.5] {2}(7);
	\end{tikzpicture} 
	\caption{A counter example of the Linear Bound heuristic}
\end{figure}

\subsubsection{Early Stopping}
If after any iteration of arc consistency, $\max_{x\in V}\nu(x) < \sup D.$ Then, $\nu(t)$ will converge to $-\infty$ for all $t.$ 
\newline Thus, we stop the algorithm and sets $\nu(t)\leftarrow -\infty,\quad \forall t$
\paragraph{Proof} suppose that in fact there is an assignment with: 
$$
-\infty < \max_{u\in V}\nu(u) < \sup D$$
We will take the $u$ with the biggest such $\nu(u).$  
\newline Now our system is a tropical max atom system, which means translations are also a polymorphism of this system, so for any assignment $\nu:V\rightarrow\mathbb{Z},\ X+t$ is also an assignment $\forall t\in \mathbb{Z}.$ With that, $\nu+\sup D-\nu(u)$ is also an assignment.
\newline This assignment has the property: $$
\forall s\in V,\quad \nu(s)+\sup D-\nu(u) \in D
$$
Which is a contradiction, as it violates the consistency of arc consistency, and the maximality of the solution with respect to the domain $D$

  
The efficiency of the Early Stopping heuristic depends on the density of the graph. Empirically, for dense graphs. the analoguous ternary max atom system has two kind of assignments:
\begin{enumerate}
	\item Either all variables are finite
	\item Either all variables are $-\infty$
\end{enumerate}
This translates back in a dense Mean Payoff setting, that the winner of the game does not depend in the starting position and the starting turn. 
\newline With that, the Early Stopping heuristic will quickly detect the second case, which is the usually hurdle of the algorithm.
\newline On the other hand, for sparse graphs, we do not have this nice distinction between finite and infinite assignments, and they can overlap, and so will make this heuristic useless in practice.

\subsection{Deployment}
After some experiments, it was very clear that vertical scaling with the number of threads is not sufficient. By analysing the running time of some samples, we estimated the total running time solving both datasets to exceed $30$ days.
\newline As a result of this, we deployed a pipeline of $24$ nodes, each with $24$ threadss working simultaneously on a partition of the dataset.
