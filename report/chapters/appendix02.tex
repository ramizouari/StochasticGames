\chapter{On Random Mean Payoff Graphs}
\label{appendix:RandomGraphs}


\section*{Introduction}
This appendix deals with some random properties of \acrshort{mpg}. The results offered by the appendix are used in the implementation and complexity analysis used in section \ref{section:MPG:Generation} of chapter \ref{section:Dataset}.

\section{Sinkless $\mathcal{D}(n,p)$ Graph}

\subsection{Property}

Let $\mathtt{P}$ be the property\footnote{Formally, a property is a just a set of graphs. In practice, it is a set that has desirable ``properties".} ``Graph has not sink".
\newline This property is increasing in the sense that:
$$
\forall H \ \text{spanning subgraph of}\ G, \quad H \in \mathtt{P}\implies G\in\mathtt{P}
$$
As a consequence:
$$
\forall n\in\mathbb{N},p,p'\in[0,1] / \quad p\le p',\quad \mathscr{P}(\mathcal{D}(n,p)\in \mathtt{P}) \le \mathscr{P}(\mathcal{D}(n,p')\in \mathtt{P})
$$
We will be interested in two properties:
\begin{itemize}
	\item The property ``Vertex $v$ has no sinks". We denote it by $\text{NoSink}(v)$.
	\item The property ``Graph $G$ has no sinks at all". We denote it by $\text{Sinkless}(G)$.
\end{itemize}

\subsection{Basic Comparison with Normal $\mathcal{D}(n,p)$}
We will calculate the expected value of $\deg v.$ By applying the law of total expectancy:
\begin{align*}
	\mathbb{E}[\deg v]&= \mathbb{E}[\deg v\mid \deg v > 0]\times \mathscr{P}(\deg v> 0) + \mathbb{E}[\deg v \mid \deg v = 0] \times \mathscr{P}(\deg v=0) \\
	&=  \mathbb{E}[\deg v\mid \text{Sinkless}(G)]\times \mathscr{P}(\text{NoSink}(v))
\end{align*}
With that:
\begin{align*}
	\mathbb{E}[\deg v \mid \text{Sinkless}(G)] &= \frac{\mathbb{E}[\deg v]}{\mathscr{P}(\text{NoSink}(v))}=\frac{np}{1-(1-p)^n} \le \frac{np}{1-e^{-1}} \\
	\mathbb{E}[\lvert \mathcal{E} \rvert] &=\sum_{v\in V} 	\mathbb{E}[\deg v \mid \text{Sinkless}(G)]= \frac{n^2p}{1-(1-p)^n} \le \frac{n^2p}{1-e^{-1}}
\end{align*}
This shows that the conditional distribution does inflict a small multiplicative bias on the expected number of edges and expected degree.
\newline This serves as an evidence that $\mathcal{D}^S(n,p)$ is similar enough to $\mathcal{D}(n,p)$

\subsection{Property Probability}
\label{section:RandomGraphs:PropertyProbability}
\begin{itemize}
\item Let $G\sim \mathcal{D}(n,p)$
\item Let $v$ a vertex of $G$
\end{itemize}
The probability that $\text{NoSink}(v)$ occurs is:
\begin{align*}
\mathcal{P}(\text{NoSink}(v))&=1- \mathcal{P}(\Adj v=\varnothing) \\ 
&=1-\mathcal{P}(\deg v=0)  \\
&= 1-(1-p)^n  
\end{align*}
Now, it is clear that the sequence of events $(\text{NoSink}(v))_{v\in V}$ is independent.
\newline With that, the probability that the whole graph is sinkless is:
\begin{align}
	\mathcal{P}(\text{Sinkless}(G))&=\mathcal{P}(\Adj v\ne \varnothing\quad\forall v \in V) \nonumber\\
	&=\mathcal{P}\left(\bigwedge_{v\in V} \text{NoSink}(v)\right) \nonumber\\
	&=\prod_{v\in V}\mathcal{P}(\text{NoSink}(v)) \nonumber \\
	&=\left(1-(1-p)^n\right)^n \label{eqn:SinklessProbability}
\end{align}
\subsection{Asymptotic Analysis For Dense $\mathcal{D}(n,p)$}
Let $c>0.$ We have for large enough $n$:
$$
(1-p)^n \le \frac{c}{n}
$$
Which implies:
$$
(1-\tfrac{c}{n})^n \le (1-(1-p)^n)^n \le 1
$$
If we take the limit, we have:
$$
  e^{-c}\le \lim_{n\rightarrow +\infty}  (1-(1-p)^n)^n \le 1 \quad \forall c>0 
$$
By tending $c$ to $0$, we get:
$$
\lim_{n\rightarrow +\infty} (1-(1-p)^n)^n=1
$$
\subsection{Asymptotic Analysis For Sparse $\mathcal{D}(n,p)$}
Let:

\begin{align*}
	f:\mathbb{R}_+^*\times \mathbb{R}_+\times \mathbb{R} & \rightarrow \mathbb{R}_+\\
	x,k,c&\rightarrow (1-g(x,k,c))^x\\
	g:\mathbb{R}_+^*\times \mathbb{R}_+\times \mathbb{R} & \rightarrow \mathbb{R}_+\\
	x,k,c&\rightarrow \left(1-\frac{k \ln x+c}{x}\right)^x
\end{align*}

By construction, $f(n,k,c)$ is the probability of a graph following $\mathcal{G}(n,\tfrac{k\ln n+c}{n})$ to contain no sink.

We have:

\begin{align*}
	\ln g(k,x,c)&=x\ln\left(1-\frac{k\ln x+c}{x}\right)\\
	&=-k\ln x-c -\frac{(k(\ln x)+c)^2}{2x}+o\left(\frac{(\ln x)^3}{x^2}\right)\\
\end{align*}
By applying the exponential function to both sides:
\begin{align*}
	g(x,k,c)&=\exp\left(-k\ln x-c -\frac{(k\ln x+c)^2}{2x}+o\left(\frac{(\ln x)^3}{x^2}\right)\right) \\
	&=\frac{e^{-c}}{x^k}\times e^{\frac{-(k\ln x+c)^2}{2x}+o\left(\frac{(\ln x)^3}{x^2}\right)}\\
	&=\frac{e^{-c}}{x^k}\left(1-\frac{(k \ln x+c)^2}{2x}+o\left(\frac{(\ln x)^3}{x^2}\right)\right)  \\
	&=\frac{e^{-c}}{x^k}-e^{-c}\frac{k^2(\ln x)^2}{2x^{k+1}}+o\left(\frac{(\ln x)^3}{x^{k+2}}\right)\\
	&=\frac{e^{-c}}{x^k}+o\left(\frac{1}{x^k}\right)\\
	\implies 1- g(x,k,c)&=1-\frac{e^{-c}}{x^{k}} +o\left(\frac{1}{x^k}\right)
\end{align*}
Now, we appy $\ln$ to both sides, and multiply by $x:$ 
\begin{align*}
	x\ln(1-g(x,k,c))&= -\frac{e^{-c}}{x^{k-1}}+o\left(\frac{1}{x^{k-1}}\right) \\
	&\sim -\frac{e^{-c}}{x^{k-1}}  \\
\end{align*}
Finally, we apply the exponential function to both sides, to get the desired estimation of $f:$
\begin{equation}
	\label{eqn:SinklessAsymptotic}
	 f(x,k,x) = e^{-\frac{e^{-c}}{x^{k-1}}+o(\frac{1}{x^{k-1}})}
\end{equation}

Now with that:
$$
\lim_{x\rightarrow +\infty} x\ln (1-g(x,k,c))=\begin{cases}
	-\infty  & \text{if} \ k\in[0,1[ \\
	-e^{-c} & \text{if}\ k=1  \\
	0 & \text{otherwise if}\ k\in \mathopen]1,+\infty\mathclose[ 
\end{cases}
$$
Finally, we can conclude that:
\begin{equation}
	\label{eqn:SinklessProbabilityLimit}
	\lim_{x\rightarrow +\infty} f(x,k)\begin{cases}
		0  & \text{if} \ k\in \mathopen[0,1\mathclose[ \\
		e^{-e^{-c}} & \text{if}\ k=1  \\
		1 & \text{otherwise if}\ k\in \mathopen]1,+\infty\mathclose[ 
	\end{cases}
\end{equation}

\section{Verification of distribution properties}
In section \ref{section:Dataset:ProposedDistributions:Properties} of chapter \ref{chapter:Dataset}, we required our \acrshort{mpg} distributions to be fair and symmetric.

We have proposed some useful weight distributions and graph distributions in table \ref{table:Distributions}. Then, we have also boldly claimed that any combination of both types of such distributions will give a \acrshort{mpg} distribution verifying both properties.

In this section, we will prove that claim. We will first start by the following notations:
\begin{itemize}
	\item Let $\mathcal{D}$ be a graph distribution.
	\item Let $\mathcal{W}$ be a symmetric weight distribution. 
	\item Let $D=(V,E)\sim \mathcal{D},$ and let $n=\lvert V \rvert$
	\item Let $W\sim \mathcal{W}$
	\item Let $s\sim \mathcal{U}(V)$
	\item Let $p\sim \mathcal{U}(\PlayerSet)$ where $\PlayerSet=\{\Max,\Min\}$
	\item Let $G=(V,E,W,s,p)$
\end{itemize}

\subsection{Symmetric}
This is immediate as $\mathcal{W}$ is symmetric by construction.
\subsection{Fairness}
\begin{align*}
	\mathscr{P}(\Max \ \text{wins in} \ G \ \text{with optimal play}) &=
	\mathscr{P}(v(G) > 0)  \\ 
	&= \mathscr{P}(v(\bar{G})<0) \\
	&= \mathscr{P}(v(V,E,-W,s,\bar{p})<0) \\
	&= \mathscr{P}(v(V,E,W,s,\bar{p})<0) \quad \text{as} \ W\sim \mathcal{W} \ \text{and}\ \mathcal{W} \ \text{is symmetric} \\  
	&= \mathscr{P}(v(V,E,W,s,p)<0) \quad \text{as} \ p \sim \mathcal{P} \\
	&=\mathscr{P}(v(G)<0) \\
		&=\mathscr{P}(\Min \ \text{wins in} \ G \ \text{with optimal play}) 
\end{align*} 
\subsection{Conclusion}
We conclude the result for any combination of distributions following the table \ref{table:Distributions} as they follow the hypotheses.

\section{Expected Mean Payoff}
\subsection{Definition}
\begin{itemize}
	\item Let $\mathcal{G}=(\mathcal{V},\mathcal{E})$ be a mean-payoff game
	\item For $u\in\mathcal{V},$ Let $\mathscr{P}(u)$ be the set of probability distributions over the set $\text{Adj}(u)$
	\item We define a fractional strategy as a function $\Phi\in \mathscr{P}$

\end{itemize}


\subsection{Matrix Form}
\begin{itemize}
	\item Let $n=\lvert \mathcal{V}\rvert$
	\item Let $u_1,\dots,u_n$ an enumeration of elements of $\mathcal{V}$
	A fractional strategy can be represented as a matrix $A$ such that:
	$$
	\mathcal{P}(\Phi(u_i)=u_j)=A_{i,j}
	$$
\end{itemize}

