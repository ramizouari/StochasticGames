## Environment with Preloaded CUDA

It appears that the HPC system comes with many available modules that can be loaded and unloaded at runtime.
We are interested in two main modules:
- CUDA module
- cuDNN module

These two modules are available at HPC and can be loaded using:
```bash
module load cuDNN/$VERSION
```

The environment variable `VERSION` loads the desired version of cuDNN.
As cuDNN also depends on CUDA, loading a cuDNN module will automatically load the compatible version of CUDA.

The partition `alpha` has a CUDA version 11.4 as suggested by the command `nvidia-smi`
With that, we will choose the version `cuDNN/8.2.2.26-CUDA-11.4.1`

This is a sample of cuDNN available versions obtained from the command `module spider cuDNN`:
![[Pasted image 20230411012932.png]]

When loading CUDA, the `CUDA_DIR` environment variable is not set, which can cause problems for TensorFlow.
To migitate this, we will add the following script at `$CONDA_PREFIX/etc/conda/activate.d/cuda_envs.sh` which will try to detect the CUDA directory from the given path.

```bash
#!/bin/bash
CUDA_DIR=$(echo $PATH | tr : '\n' | grep -i CUDA | sed -E "s/(\/nvvm\/bin|\/bin)//g" | ( while read dir; do
	if [ -d $dir/nvvm ]; then
		echo $dir
	fi
done ) | sort -u )
LINE_COUNT=$(echo $CUDA_DIR | wc -l)
CUDA_DIR=$(echo $CUDA_DIR | head -n 1)
if [ -z $CUDA_DIR ]; then
	echo "No CUDA directory detected" 1>&2
else 
	if [ $LINE_COUNT -ge 2 ]; then
		echo "More than one CUDA directory detected. Choosing the first one..."
	fi
	echo "choosing $CUDA_DIR" as the base directory of CUDA
	export CUDA_DIR
	export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CUDA_DIR
fi
```


## Adding Certificate
So that the server's certificate can be trusted, we extracted the certificate from the browser under the name `rami_zouari_tu.pem`.
And then, copied it to the CA store as follow:
```bash
sudo cp rami_zouari_tu.pem /etc/pki/ca-trust/source/anchors/
```

And then, we updated the certificate store as follow:
```bash
sudo update-ca-trust
```



## Installing Jupyter Kernels

To access each environment via ZIH's provided Jupyter, we can add jupyter kernels. 
First we will add the following files
```bash
mkdir -p $HOME/shellbin
cd $HOME/shellbin
touch python-with-conda python-with-conda-loaded-modules
chmod u+x python-with-conda python-with-conda-loaded-modules
```

We will populate `python-with-conda` with the following content:
```sh
#!/bin/sh
source $HOME/.bashrc
conda activate $WORKSPACE_CONDA
python "$@"
```

We will then populate `pyhon-with-conda-loaded-modules` with the following content
```sh
#!/bin/sh
#Activate the work environment without setting the local runtime
SKIP_RUNTIME=1 source $HOME/.workenv
source $WORKSPACE_CONDA_LOADED/etc/conda/activate.d/cuda_env.sh
$WORKSPACE_CONDA_LOADED/bin/python "$@"
```

Now we will install both kernels via the command:
```bash
python -m ipykernel install --user --name python_3_10_conda --display-name "Python 3.10 (conda)"
python -m ipykernel install --user --name python_3_10_conda_loaded_modules --display-name "Python 3.10 (conda + modules)"
```

Finally, we will modify each kernel's executable to the desired one:
```bash
HOME_ESC=$(echo $HOME | sed "s/\//\\\\\//g")
sed -E "s/\".*python\"/\"$HOME_ESC\/shellbin\/python-with-conda\"/g" -i $HOME/.local/share/jupyter/kernels/python_3_10_conda/kernel.json
sed -E "s/\".*python\"/\"$HOME_ESC\/shellbin\/python-with-conda-loaded-modules\"/g" -i $HOME/.local/share/jupyter/kernels/python_3_10_conda_loaded_modules/kernel.json
```
