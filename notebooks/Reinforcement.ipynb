{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1f188a3a-1a4a-4368-b2c4-1816a9e37719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "074abc3c-371c-458e-9884-7360a4ec6be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardGameEnv(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        if self._episode_ended:\n",
    "          # The last action ended the episode. Ignore the current action and start\n",
    "          # a new episode.\n",
    "            return self.reset()\n",
    "\n",
    "        # Make sure episodes don't go on forever.\n",
    "        if action == 1:\n",
    "            self._episode_ended = True\n",
    "        elif action == 0:\n",
    "            new_card = np.random.randint(1, 11)\n",
    "            self._state += new_card\n",
    "        else:\n",
    "            raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "        if self._episode_ended or self._state >= 21:\n",
    "            reward = self._state - 21 if self._state <= 21 else -21\n",
    "            return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "        else:\n",
    "            return ts.transition(\n",
    "              np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f0e7220c-4d29-49b9-8302-02883cb6d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = CardGameEnv()\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc71781-5bce-4538-bfda-a0a247e72ebe",
   "metadata": {},
   "source": [
    "## 2. MPG Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "99a08238-94c8-49c7-9ae6-619195e1bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpg.games import mpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "fe3b00ac-bf51-4eff-aef8-f1133467addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpg.games import strategy,mpg\n",
    "from mpg.rl import model_free,environment as rl_env\n",
    "import importlib\n",
    "importlib.reload(strategy)\n",
    "importlib.reload(mpg)\n",
    "importlib.reload(model_free)\n",
    "importlib.reload(rl_env)\n",
    "G=mpg.mpg_from_file(\"data/test01.in\",ignore_header=1)\n",
    "G\n",
    "environment = rl_env.MPGEnvironment(G,0,0,10,bad_action_penalty=-40)\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "2bb6e23e-4fa9-4bec-80c5-49d34c162457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.reward_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "69c18508-d3b6-41d1-93a0-bdb8f91e44c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = rl_env.MPGEnvironment(G,1,0,max_turns=100,bad_action_penalty=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "e7c5a12d-fc56-4a35-9581-9b48512b016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_env=rl_env.FixedStrategyMPGEnvironment(environment,strategy.GreedyStrategy(environment.graph,turn=mpg.MeanPayoffGraph.player1))\n",
    "fixed_env.reset()\n",
    "agent=model_free.RLearningAgent(fixed_env)\n",
    "fo_env=rl_env.FullyObservableMPGEnvironment(fixed_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "53f41ec1-eed2-4769-bb25-ed99ce012f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51416da4-c2b4-4094-8be8-b7e3ba3b8cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d73cbe-371d-4e71-a377-85152ad4009b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa60ee-9a5f-4af3-8e9c-0a3a7e38f2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3dfb546b-8732-4abc-8896-c321992e74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment._vertex=np.array(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "85b13569-779a-42dd-a437-8f48aa510c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, 5: 5, 3: 5, 4: 5}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{1:2,5:5,3:5,4:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5d3abf8f-970a-44e6-962f-f35e7848b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A={0:np.zeros(5),1:np.zeros(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "97bc5002-c85e-467c-8cd1-12520662e978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "35401ede-5ea1-41ae-854d-523ac1fba58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1dbe06bbf294780a2793d0fbe6fabd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MPGVisualisation(layout=Layout(height='500px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpg.visualisation import game as vgame\n",
    "VG=vgame.MPGVisualisation(G)\n",
    "VG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "48b7587a-d4d1-4aca-a8be-6ceb1bce0753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mpg.rl.architectures.example' from '/home/ramizouari/Academic/AI/MeanPayOffGames/notebooks/mpg/rl/architectures/example.py'>"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(rl_env)\n",
    "import mpg.rl.driver as rl_driver\n",
    "importlib.reload(rl_driver)\n",
    "import tf_agents as tfa\n",
    "import mpg.rl.replay_buffers as rl_replay\n",
    "importlib.reload(rl_replay)\n",
    "import mpg.rl.agents as rl_agents\n",
    "importlib.reload(rl_agents)\n",
    "import mpg.rl.architectures.example as rl_arch\n",
    "importlib.reload(rl_arch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "1c65aa2c-e06e-44cc-a71c-9369436afab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArraySpec(shape=(2, 8, 8), dtype=dtype('float32'), name=None)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(rl_env)\n",
    "E=rl_env.MPGMatrixExtractor(matrix=\"both\",graph_size=8)\n",
    "E.get_env_specs(fixed_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "1d7bfa91-8c3b-43e2-9447-49cae8407420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  5.,  0.,  0.,  0.,  4.,  0.,  0.],\n",
       "        [ 0.,  0., -7.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  5.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [-3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0., -3.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  3.,  0.,  0.,  0.],\n",
       "        [ 5.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E(fixed_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "28758e61-549b-471f-af4c-49fc57c60701",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam()\n",
    "\n",
    "converter=rl_env.MPGTrajectoryConverter(fixed_env,E)\n",
    "\n",
    "qnet=tfa.networks.q_network.QNetwork(\n",
    "    input_tensor_spec=converter.data_spec[\"environment\"],\n",
    "    action_spec=fixed_env.action_spec(),\n",
    "    preprocessing_layers=None,\n",
    "    preprocessing_combiner=None,\n",
    "    conv_layer_params=None,\n",
    "    fc_layer_params=(75, 40),\n",
    "    dropout_layer_params=None,\n",
    "    activation_fn=tf.keras.activations.relu,\n",
    "    kernel_initializer=None,\n",
    "    batch_squash=True,\n",
    "    dtype=tf.float32,\n",
    "    q_layer_activation_fn=None,\n",
    "    name='QNetwork'\n",
    ")\n",
    "\n",
    "net=rl_arch.MPGNetworkExample(fo_env.observation_spec(),fo_env.count_vertices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "18f01eb4-acb2-457c-9726-7b3bca304136",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=tfa.agents.DqnAgent(\n",
    "    time_step_spec=fo_env.time_step_spec(),\n",
    "    action_spec= fo_env.action_spec(),\n",
    "    q_network= net,\n",
    "    optimizer= optimizer,\n",
    "    observation_and_action_constraint_splitter= None,\n",
    "    epsilon_greedy= 0.1,\n",
    "    n_step_update = 1,\n",
    "#    training_data_spec= converter.data_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "6762ad1b-36bc-4bd4-a0d8-44b8abf18be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_TupleWrapper(Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(7, dtype=int32)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': DictWrapper({'state': BoundedTensorSpec(shape=(), dtype=tf.int32, name='observation', minimum=array(0, dtype=int32), maximum=array(7, dtype=int32)), 'environment': TensorSpec(shape=(2, 8, 8), dtype=tf.float32, name='environment')}),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.training_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "c9f339bb-c1c1-49a2-b2ea-4309ead71cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_agent=rl_agents.FullyObservableMPGAgentWrapper(agent,fixed_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c0a24b-f26d-4e12-a1f6-35736cac2919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "2f8f6c7b-aa44-4842-bb25-21140e6672ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.agents.dqn.dqn_agent.DqnAgent at 0x7fc2244a8b80>"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "dbbe210f-aba1-42af-9b45-2ef3d8500b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buffer=rl_replay.MPGMatrixBuffer(converter.data_spec[\"environment\"],10)\n",
    "driver=rl_driver.MPGDriver(fo_env,agent.collect_policy,total_observers=[],partial_observers=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "b9558aa8-671c-49bf-9707-088b1f6dedc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'concatenate_7' (type Concatenate).\n\n{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [2,64] vs. shape[1] = [1,1] [Op:ConcatV2] name: concat\n\nCall arguments received by layer 'concatenate_7' (type Concatenate):\n  • inputs=['tf.Tensor(shape=(2, 64), dtype=float32)', 'tf.Tensor(shape=(1, 1), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[429], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfo_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/drivers/py_driver.py:118\u001b[0m, in \u001b[0;36mPyDriver.run\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mbatched \u001b[38;5;129;01mand\u001b[39;00m time_step\u001b[38;5;241m.\u001b[39mis_first() \u001b[38;5;129;01mand\u001b[39;00m num_episodes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    116\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mget_initial_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m action_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m next_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action_step\u001b[38;5;241m.\u001b[39maction)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# When using observer (for the purpose of training), only the previous\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# policy_state is useful. Therefore substitube it in the PolicyStep and\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# consume it w/ the observer.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/policies/tf_policy.py:324\u001b[0m, in \u001b[0;36mTFPolicy.action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_automatic_state_reset:\n\u001b[1;32m    323\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_reset_state(time_step, policy_state)\n\u001b[0;32m--> 324\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[43maction_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_action\u001b[39m(action, action_spec):\n\u001b[1;32m    327\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action_spec, tensor_spec\u001b[38;5;241m.\u001b[39mBoundedTensorSpec):\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/utils/common.py:188\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m check_tf1_allowed()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_eager_been_enabled():\n\u001b[1;32m    186\u001b[0m   \u001b[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[1;32m    187\u001b[0m   \u001b[38;5;66;03m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_variables_enabled():\n\u001b[1;32m    190\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/policies/epsilon_greedy_policy.py:115\u001b[0m, in \u001b[0;36mEpsilonGreedyPolicy._action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, time_step, policy_state, seed):\n\u001b[1;32m    114\u001b[0m   seed_stream \u001b[38;5;241m=\u001b[39m tfp\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mSeedStream(seed\u001b[38;5;241m=\u001b[39mseed, salt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon_greedy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m   greedy_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m   random_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_policy\u001b[38;5;241m.\u001b[39maction(time_step, (), seed_stream())\n\u001b[1;32m    118\u001b[0m   outer_shape \u001b[38;5;241m=\u001b[39m nest_utils\u001b[38;5;241m.\u001b[39mget_outer_shape(time_step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_step_spec)\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/policies/tf_policy.py:324\u001b[0m, in \u001b[0;36mTFPolicy.action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_automatic_state_reset:\n\u001b[1;32m    323\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_reset_state(time_step, policy_state)\n\u001b[0;32m--> 324\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[43maction_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_action\u001b[39m(action, action_spec):\n\u001b[1;32m    327\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action_spec, tensor_spec\u001b[38;5;241m.\u001b[39mBoundedTensorSpec):\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/utils/common.py:188\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m check_tf1_allowed()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_eager_been_enabled():\n\u001b[1;32m    186\u001b[0m   \u001b[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[1;32m    187\u001b[0m   \u001b[38;5;66;03m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_variables_enabled():\n\u001b[1;32m    190\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/policies/tf_policy.py:560\u001b[0m, in \u001b[0;36mTFPolicy._action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of `action`.\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;03m    `info`: Optional side information such as action log probabilities.\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    559\u001b[0m seed_stream \u001b[38;5;241m=\u001b[39m tfp\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mSeedStream(seed\u001b[38;5;241m=\u001b[39mseed, salt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_agents_tf_policy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 560\u001b[0m distribution_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=wrong-arg-types\u001b[39;00m\n\u001b[1;32m    561\u001b[0m actions \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m d: reparameterized_sampling\u001b[38;5;241m.\u001b[39msample(d, seed\u001b[38;5;241m=\u001b[39mseed_stream()),\n\u001b[1;32m    563\u001b[0m     distribution_step\u001b[38;5;241m.\u001b[39maction)\n\u001b[1;32m    564\u001b[0m info \u001b[38;5;241m=\u001b[39m distribution_step\u001b[38;5;241m.\u001b[39minfo\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/policies/greedy_policy.py:80\u001b[0m, in \u001b[0;36mGreedyPolicy._distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour network\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms distribution does not implement mode \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaking it incompatible with a greedy policy.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     76\u001b[0m                     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m     78\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tfp\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mDeterministic(loc\u001b[38;5;241m=\u001b[39mgreedy_action)\n\u001b[0;32m---> 80\u001b[0m distribution_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m policy_step\u001b[38;5;241m.\u001b[39mPolicyStep(\n\u001b[1;32m     83\u001b[0m     tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(dist_fn, distribution_step\u001b[38;5;241m.\u001b[39maction),\n\u001b[1;32m     84\u001b[0m     distribution_step\u001b[38;5;241m.\u001b[39mstate, distribution_step\u001b[38;5;241m.\u001b[39minfo)\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/policies/tf_policy.py:403\u001b[0m, in \u001b[0;36mTFPolicy.distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_automatic_state_reset:\n\u001b[1;32m    402\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_reset_state(time_step, policy_state)\n\u001b[0;32m--> 403\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit_log_probability:\n\u001b[1;32m    405\u001b[0m   \u001b[38;5;66;03m# This here is set only for compatibility with info_spec in constructor.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m   info \u001b[38;5;241m=\u001b[39m policy_step\u001b[38;5;241m.\u001b[39mset_log_probability(\n\u001b[1;32m    407\u001b[0m       step\u001b[38;5;241m.\u001b[39minfo,\n\u001b[1;32m    408\u001b[0m       tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    409\u001b[0m           \u001b[38;5;28;01mlambda\u001b[39;00m _: tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m0.\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    410\u001b[0m           policy_step\u001b[38;5;241m.\u001b[39mget_log_probability(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_spec)))\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/policies/q_policy.py:156\u001b[0m, in \u001b[0;36mQPolicy._distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observation_and_action_constraint_splitter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m   network_observation, mask \u001b[38;5;241m=\u001b[39m observation_and_action_constraint_splitter(\n\u001b[1;32m    154\u001b[0m       network_observation)\n\u001b[0;32m--> 156\u001b[0m q_values, policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_q_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork_observation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m logits \u001b[38;5;241m=\u001b[39m q_values\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observation_and_action_constraint_splitter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m   \u001b[38;5;66;03m# Overwrite the logits for invalid actions to logits.dtype.min.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/networks/network.py:427\u001b[0m, in \u001b[0;36mNetwork.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mis_tensor(network_state)\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m network_state \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, ())\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_state\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m call_argspec\u001b[38;5;241m.\u001b[39margs\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m call_argspec\u001b[38;5;241m.\u001b[39mkeywords):\n\u001b[1;32m    425\u001b[0m   normalized_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 427\u001b[0m outputs, new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnormalized_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=attribute-error  # typed-keras\u001b[39;00m\n\u001b[1;32m    429\u001b[0m nest_utils\u001b[38;5;241m.\u001b[39massert_matching_dtypes_and_inner_shapes(\n\u001b[1;32m    430\u001b[0m     new_state,\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_spec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m     tensors_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`new_state`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    435\u001b[0m     specs_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`state_spec`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, new_state\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Academic/AI/MeanPayOffGames/notebooks/mpg/rl/architectures/example.py:18\u001b[0m, in \u001b[0;36mMPGNetworkExample.call\u001b[0;34m(self, observations, step_type, network_state, training)\u001b[0m\n\u001b[1;32m     16\u001b[0m G\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(G)\n\u001b[1;32m     17\u001b[0m A\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mreshape(A,[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m]),dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 18\u001b[0m Z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43mA\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m Z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(Z)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions(Z),network_state\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'concatenate_7' (type Concatenate).\n\n{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [2,64] vs. shape[1] = [1,1] [Op:ConcatV2] name: concat\n\nCall arguments received by layer 'concatenate_7' (type Concatenate):\n  • inputs=['tf.Tensor(shape=(2, 64), dtype=float32)', 'tf.Tensor(shape=(1, 1), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "driver.run(fo_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "f3aff162-250b-4ce3-abfc-49a70002aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=tfa.trajectories.time_step.TimeStep(step_type=np.array(tfa.trajectories.time_step.StepType.FIRST),observation=np.array(0),reward=np.array(0),discount=np.array(1))\n",
    "s=tfa.trajectories.PolicyStep(action=np.array(0))\n",
    "t1=tfa.trajectories.time_step.TimeStep(step_type=tfa.trajectories.time_step.StepType.FIRST,observation=np.array(0),reward=0,discount=1)\n",
    "#driver.observers[0](tfa.trajectories.from_transition(t0,s,t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "80ecb9d2-b1b4-4df0-b927-7b9e4661f6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': BoundedArraySpec(shape=(), dtype=dtype('int32'), name='observation', minimum=0, maximum=7),\n",
       " 'environment': ArraySpec(shape=(2, 8, 8), dtype=dtype('float32'), name='environment')}"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo_env.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d4ab7772-0da8-4eb5-8b86-099b94944f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer.gather_all()[\"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "21a5089e-0c72-4975-b0bc-e095dd441b67",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[272], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/replay_buffers/replay_buffer.py:83\u001b[0m, in \u001b[0;36mReplayBuffer.add_batch\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, items):\n\u001b[1;32m     73\u001b[0m   \u001b[38;5;124;03m\"\"\"Adds a batch of items to the replay buffer.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    Adds `items` to the replay buffer.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stochastic_games/lib/python3.10/site-packages/tf_agents/replay_buffers/py_uniform_replay_buffer.py:100\u001b[0m, in \u001b[0;36mPyUniformReplayBuffer._add_batch\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, items):\n\u001b[1;32m     99\u001b[0m   outer_shape \u001b[38;5;241m=\u001b[39m nest_utils\u001b[38;5;241m.\u001b[39mget_outer_array_shape(items, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_spec)\n\u001b[0;32m--> 100\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mouter_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPyUniformReplayBuffer only supports a batch \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    102\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize of 1, but received `items` with batch \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    103\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(outer_shape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m    105\u001b[0m   item \u001b[38;5;241m=\u001b[39m nest_utils\u001b[38;5;241m.\u001b[39munbatch_nested_array(items)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "buffer.add_batch(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fc2a9221-ff21-42fd-8b08-084ee9e69384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfa.utils.nest_utils.get_outer_array_shape(t0, buffer._data_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7e0e3f7c-44bd-4215-9769-94e54f7793be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': ArraySpec(shape=(), dtype=dtype('int32'), name=None),\n",
       " 'environment': ArraySpec(shape=(2, 8, 8), dtype=dtype('float32'), name=None)}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer._data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f73805c4-a91e-4e36-8e67-9b26065fbb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfa.utils.nest_utils.get_outer_array_shape(np.array([[5,3],[5,2]],dtype=np.int32),spec=tfa.specs.ArraySpec(shape=[2],dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "69bf8ddd-5709-47ce-be81-094ae576c361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': array([1]),\n",
       " 'observation': array([0]),\n",
       " 'reward': array([0]),\n",
       " 'step_type': array([0], dtype=int32)})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "f24ca11d-2e3a-497d-94c2-33d8c5ba9e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(7, dtype=int32)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': BoundedTensorSpec(shape=(), dtype=tf.int32, name='observation', minimum=array(0, dtype=int32), maximum=array(7, dtype=int32)),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d6a242-ce25-4aba-911e-6e8960023333",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfa.utils.nest_utils.get_outer_array_shape(np.array([[5,3],[5,2]],dtype=np.int32),spec=tfa.specs.ArraySpec(shape=[2],dtype=np.int32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
